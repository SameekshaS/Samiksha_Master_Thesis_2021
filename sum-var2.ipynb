{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAIR USE POLICY\n",
    "**Please do not leave your Jupyter lab server idle for extended periods of time.** The Jupyter process, active Python kernels, and especially running Spark contexts, claim a minimum amount of cluster resources. These add up and will get starve resources of others eventually. Leaving your environment idle for a few hours (e.g., over lunch) is fine. But letting it idle overnight or for multiple days in which you are not actively using the cluster is not. You can kill the server from your SSH session, by pressing ctrl+c repeatedly, or by selecting *File->Shutdown* from the menu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta, date\n",
    "import pprint\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "\n",
    "import pyspark\n",
    "# Find Spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "\n",
    "from datetime import datetime, timedelta, date\n",
    "import datetime as dt\n",
    "from datetime import timedelta as td\n",
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import re\n",
    "import pyarrow as pa\n",
    "import copy\n",
    "from pyspark.sql.types import DateType\n",
    "from string import digits\n",
    "from dateutil import parser\n",
    "\n",
    "import nltk\n",
    "import pyspark\n",
    "from pyspark.sql import *  \n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as F \n",
    "from pyspark.sql import functions as F, Window\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import mean as _mean\n",
    "from pyspark.sql.window import Window as W\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, LongType\n",
    "from scipy import stats\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "\n",
    "\n",
    "from functools import reduce\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Spark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkConf\n",
    "APP_NAME = \"apwg-var2-app\"\n",
    "\n",
    "spark_conf = pyspark.SparkConf().setAppName(APP_NAME).setMaster(\"yarn\").set(\n",
    "    \"spark.submit.deployMode\", \"client\"\n",
    ").set(\"spark.sql.parquet.binaryAsString\", \"true\"\n",
    ").set(\"spark.dynamicAllocation.maxExecutors\", \"16\"\n",
    ").set(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.0.0\"\n",
    ").set(\"spark.sql.debug.maxToStringFields\", \"1000\"\n",
    ").set(\"spark.executor.memory\", \"7G\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start SparkContext\n",
    "1. This may take a minute to complete\n",
    "2. You should not (and cannot) start two Spark contexts. If you accidentally run this cell twice or get stuck somehow, restart your Python kernel from the menu above.\n",
    "3. Please **stop your Spark context** when idling for extended periods of time (see code at bottom of notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[{}] Starting Spark context.\".format(datetime.now().replace(microsecond=0)))\n",
    "\n",
    "# SparkContext\n",
    "sc = pyspark.SparkContext(conf=spark_conf)\n",
    "\n",
    "# SQLContext\n",
    "sqlc = pyspark.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "APWG_CLEAN_DATA_CONVERTED_BASE = \"PATH TO DATA-RECORDS\"\n",
    "\n",
    "INTEREST_DATE_START = datetime(2017, 12, 18)\n",
    "INTEREST_DATE_END   = datetime(2019, 8, 16)\n",
    "\n",
    "# Read JSON files into Spark DF\n",
    "clean_mails_df = sqlc.read.json(APWG_CLEAN_DATA_CONVERTED_BASE, multiLine=True).withColumn(\n",
    "    \"parsed_date\", F.from_unixtime(F.col(\"date_received\")).cast(\"date\")\n",
    ").filter(\n",
    "    # Filter date range of interest\n",
    "    (F.col(\"parsed_date\") >= INTEREST_DATE_START.date().isoformat()) &\n",
    "    (F.col(\"parsed_date\") <= INTEREST_DATE_END.date().isoformat())\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter emails in english language\n",
    "original_df = clean_mails_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = original_df.filter(original_df.language == \"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1_df = original_df   \n",
    "h1_df = h1_df.select('parsed_date', 'id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol='body_words', outputCol='words_clean')\n",
    "h2_df = remover.transform(original_df)\n",
    "\n",
    "h2_df = h2_df.drop(\"body_words\")\n",
    "\n",
    "word_list=['unsubscribed', 'hack', 'takedown', 'password', 'transparent',\\\n",
    "           'attempt', 'redirect', 'impersonate', 'network', 'obsolete', 'illegal', 'damage', 'edit',\\\n",
    "           'unauthenticated', 'initial', 'survey', 'collect', 'victim', 'detect', 'recharge', 'test',\\\n",
    "           'attachment', 'claim', 'profitable', 'virus', 'fraudulent', 'revalidation', 'link', 'description']\n",
    "\n",
    "#array_intersect function requires two arrays as arguments, create array from the list of given values:\n",
    "list_col = F.array(*[F.lit(cl) for cl in word_list])\n",
    "h2_df = h2_df.filter(F.size(F.array_intersect(F.col(\"words_clean\"), list_col)) > 0)\n",
    "h2_df = h2_df.select('parsed_date', 'id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3_df = original_df.filter(original_df.email_has_attachments == \"1\")\n",
    "h3_df = h3_df.select('parsed_date', 'id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# announcement date of ddos attack\n",
    "\n",
    "ddos_list = ['2018-01-17', '2018-01-27', '2018-02-08', '2018-03-01', '2018-03-06', '2018-05-14', '2018-05-24', '2018-07-30',\\\n",
    "             '2019-01-16','2019-01-31', '2019-02-23', '2019-03-22', '2019-04-16', '2019-06-12', '2019-08-08']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#partitionBy is used to shuffle data before applying the functions\n",
    "def hypo_1(df, day):\n",
    "    \n",
    "    h1_df1 = (df.filter(f\"parsed_date < '{day}' and parsed_date > '{day}' - interval 20 days\")\n",
    "             .withColumn('rn', F.dense_rank().over(Window.orderBy(F.desc('parsed_date'))))\n",
    "             .filter('rn <= 7')\n",
    "             .drop('rn')\n",
    "             .withColumn('count_before', F.count('id').over(Window.partitionBy('parsed_date')))\n",
    "             .orderBy('parsed_date').withColumn('ddos_date', F.lit(day))\n",
    "          )\n",
    "\n",
    "    h1_df2 = (df.filter(f\"parsed_date < '{day}' + interval 20 days and parsed_date > '{day}'\")\n",
    "             .withColumn('rn', F.dense_rank().over(Window.orderBy('parsed_date')))\n",
    "             .filter('rn <= 7')\n",
    "             .drop('rn')\n",
    "             .withColumn('count_after', F.count('id').over(Window.partitionBy('parsed_date')))\n",
    "             .orderBy('parsed_date').withColumn('ddos_date', F.lit(day))\n",
    "          )\n",
    "\n",
    "\n",
    "    \n",
    "    return [h1_df1, h1_df2]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypo_2(df, day):\n",
    "    \"\"\"\n",
    "    Example usage: df_list = hypo_2(df, '2017-12-18', 15)\n",
    "    Returns a list of 2 dataframes.\n",
    "    \"\"\"\n",
    "    h2_df1 = (df.filter(f\"parsed_date < '{day}' and parsed_date > '{day}' - interval 20 days\")\n",
    "             .withColumn('rn', F.dense_rank().over(Window.orderBy(F.desc('parsed_date'))))\n",
    "             .filter('rn <= 15')\n",
    "             .drop('rn')\n",
    "             .withColumn('count_before', F.count('id').over(Window.partitionBy('parsed_date')))\n",
    "             .orderBy('parsed_date').withColumn('ddos_date', F.lit(day))\n",
    "          )\n",
    " \n",
    "    \n",
    "    h2_df2 = (df.filter(f\"parsed_date < '{day}' + interval 20 days and parsed_date > '{day}'\")\n",
    "             .withColumn('rn', F.dense_rank().over(Window.orderBy('parsed_date')))\n",
    "             .filter('rn <= 15')\n",
    "             .drop('rn')\n",
    "             .withColumn('count_after', F.count('id').over(Window.partitionBy('parsed_date')))\n",
    "             .orderBy('parsed_date').withColumn('ddos_date', F.lit(day))\n",
    "          )\n",
    "\n",
    "    return [h2_df1, h2_df2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypo_3(df, day):\n",
    "    \"\"\"\n",
    "    Example usage: df_list = hypo_3(df, '2017-12-18', 15)\n",
    "    Returns a list of 2 dataframes.\n",
    "    \"\"\"\n",
    "    h3_df1 = (df.filter(f\"parsed_date < '{day}' and parsed_date > '{day}' - interval 20 days\")\n",
    "             .withColumn('rn', F.dense_rank().over(Window.orderBy(F.desc('parsed_date'))))\n",
    "             .filter('rn <= 15')\n",
    "             .drop('rn')\n",
    "             .withColumn('count_before', F.count('id').over(Window.partitionBy('parsed_date')))\n",
    "             .orderBy('parsed_date').withColumn('ddos_date', F.lit(day))\n",
    "          )\n",
    " \n",
    "    \n",
    "    h3_df2 = (df.filter(f\"parsed_date < '{day}' + interval 20 days and parsed_date > '{day}'\")\n",
    "             .withColumn('rn', F.dense_rank().over(Window.orderBy('parsed_date')))\n",
    "             .filter('rn <= 15')\n",
    "             .drop('rn')\n",
    "             .withColumn('count_after', F.count('id').over(Window.partitionBy('parsed_date')))\n",
    "             .orderBy('parsed_date').withColumn('ddos_date', F.lit(day))\n",
    "          )\n",
    "\n",
    "    \n",
    "    \n",
    "    return [h3_df1, h3_df2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h1_df1_list = []\n",
    "h1_df2_list = []\n",
    "\n",
    "for d in ddos_list:\n",
    "    h1_df1, h1_df2 = hypo_1(h1_df, d)\n",
    "    \n",
    "    h1_df1 = h1_df1.dropDuplicates(['parsed_date', 'count_before'])\n",
    "    h1_df2 = h1_df2.dropDuplicates(['parsed_date', 'count_after'])\n",
    "    \n",
    "    h1_df1 = h1_df1.withColumn(\"sum_before\", F.sum(\"count_before\").over(Window.partitionBy(\"ddos_date\")))\n",
    "    h1_df2 = h1_df2.withColumn(\"sum_after\", F.sum(\"count_after\").over(Window.partitionBy(\"ddos_date\")))\n",
    "    \n",
    "    \n",
    "    h1_df1_list.append(h1_df1)\n",
    "    h1_df2_list.append(h1_df2)\n",
    "    \n",
    "    \n",
    "hypo1_df1 = reduce(DataFrame.unionAll, h1_df1_list)\n",
    "hypo1_df2 = reduce(DataFrame.unionAll, h1_df2_list)\n",
    "\n",
    "\n",
    "hypo1_df1 = (date_df.join(hypo1_df1, 'ddos_date', 'left')\n",
    "                 .fillna(0, 'count_before')\n",
    "                 .groupBy('ddos_date')\n",
    "                 .agg(F.sum('count_before').alias('sum_before'))\n",
    "         )\n",
    "hypo1_df2 = (date_df.join(hypo1_df2, 'ddos_date', 'left')\n",
    "                 .fillna(0, 'count_after')\n",
    "                 .groupBy('ddos_date')\n",
    "                 .agg(F.sum('count_after').alias('sum_after'))\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2_df1_list = []\n",
    "h2_df2_list = []\n",
    "\n",
    "for d in ddos_list:\n",
    "    h2_df1, h2_df2 = hypo_2(h2_df, d)\n",
    "    \n",
    "    h2_df1 = h2_df1.dropDuplicates(['parsed_date', 'count_before'])\n",
    "    h2_df2 = h2_df2.dropDuplicates(['parsed_date', 'count_after'])\n",
    "    \n",
    "    h2_df1 = h2_df1.withColumn(\"sum_before\", F.sum(\"count_before\").over(Window.partitionBy(\"ddos_date\")))\n",
    "    h2_df2 = h2_df2.withColumn(\"sum_after\", F.sum(\"count_after\").over(Window.partitionBy(\"ddos_date\")))\n",
    "    \n",
    "    h2_df1_list.append(h2_df1)\n",
    "    h2_df2_list.append(h2_df2)\n",
    "    \n",
    "    \n",
    "hypo2_df1 = reduce(DataFrame.unionAll, h2_df1_list)\n",
    "hypo2_df2 = reduce(DataFrame.unionAll, h2_df2_list)\n",
    "\n",
    "hypo2_df1 = (date_df.join(hypo2_df1, 'ddos_date', 'left')\n",
    "                 .fillna(0, 'count_before')\n",
    "                 .groupBy('ddos_date')\n",
    "                 .agg(F.sum('count_before').alias('sum_before'))\n",
    "         )\n",
    "hypo2_df2 = (date_df.join(hypo2_df2, 'ddos_date', 'left')\n",
    "                 .fillna(0, 'count_after')\n",
    "                 .groupBy('ddos_date')\n",
    "                 .agg(F.sum('count_after').alias('sum_after'))\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3_df1_list = []\n",
    "h3_df2_list = []\n",
    "\n",
    "for d in ddos_list:\n",
    "    h3_df1, h3_df2 = hypo_3(h3_df, d)\n",
    "    \n",
    "    h3_df1 = h3_df1.dropDuplicates(['parsed_date', 'count_before'])\n",
    "    h3_df2 = h3_df2.dropDuplicates(['parsed_date', 'count_after'])\n",
    "    \n",
    "    h3_df1 = h3_df1.withColumn(\"sum_before\", F.sum(\"count_before\").over(Window.partitionBy(\"ddos_date\")))\n",
    "    h3_df2 = h3_df2.withColumn(\"sum_after\", F.sum(\"count_after\").over(Window.partitionBy(\"ddos_date\")))\n",
    "    \n",
    "    h3_df1_list.append(h3_df1)\n",
    "    h3_df2_list.append(h3_df2)\n",
    "    \n",
    "\n",
    "hypo3_df1 = reduce(DataFrame.unionAll, h3_df1_list)\n",
    "hypo3_df2 = reduce(DataFrame.unionAll, h3_df2_list)\n",
    "\n",
    "hypo3_df1 = (date_df.join(hypo3_df1, 'ddos_date', 'left')\n",
    "                 .fillna(0, 'count_before')\n",
    "                 .groupBy('ddos_date')\n",
    "                 .agg(F.sum('count_before').alias('sum_before'))\n",
    "         )\n",
    "hypo3_df2 = (date_df.join(hypo3_df2, 'ddos_date', 'left')\n",
    "                 .fillna(0, 'count_after')\n",
    "                 .groupBy('ddos_date')\n",
    "                 .agg(F.sum('count_after').alias('sum_after'))\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------------+----------+----------+\n",
      "|parsed_date|     id|count_before| ddos_date|sum_before|\n",
      "+-----------+-------+------------+----------+----------+\n",
      "| 2018-01-16|1520036|        1228|2018-01-17|      3084|\n",
      "| 2018-01-14|1516364|         793|2018-01-17|      3084|\n",
      "| 2018-01-15|1518451|        1063|2018-01-17|      3084|\n",
      "| 2018-01-24|1536787|        1301|2018-01-27|      3533|\n",
      "| 2018-01-26|1539203|        1106|2018-01-27|      3533|\n",
      "| 2018-01-25|1537091|        1126|2018-01-27|      3533|\n",
      "| 2019-01-03|2325105|        1250|2019-01-16|      1298|\n",
      "| 2019-01-04|2327127|          48|2019-01-16|      1298|\n",
      "+-----------+-------+------------+----------+----------+\n",
      "\n",
      "+-----------+-------+-----------+----------+---------+\n",
      "|parsed_date|     id|count_after| ddos_date|sum_after|\n",
      "+-----------+-------+-----------+----------+---------+\n",
      "| 2018-01-19|1525745|       1037|2018-01-17|     3252|\n",
      "| 2018-01-20|1527874|       1045|2018-01-17|     3252|\n",
      "| 2018-01-18|1523955|       1170|2018-01-17|     3252|\n",
      "| 2018-01-29|1545715|       1271|2018-01-27|     3405|\n",
      "| 2018-01-30|1546220|       1334|2018-01-27|     3405|\n",
      "| 2018-01-28|1542595|        800|2018-01-27|     3405|\n",
      "| 2019-02-01|2398793|       1791|2019-01-16|     1845|\n",
      "| 2019-02-02|2401595|         54|2019-01-16|     1845|\n",
      "| 2019-02-01|2398793|       1791|2019-01-31|     3454|\n",
      "| 2019-02-02|2401595|         54|2019-01-31|     3454|\n",
      "| 2019-02-14|2438595|       1609|2019-01-31|     3454|\n",
      "| 2019-06-26|2947690|       1594|2019-06-12|     1786|\n",
      "| 2019-06-27|2950214|        192|2019-06-12|     1786|\n",
      "+-----------+-------+-----------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hypo1_df1.orderBy('ddos_date').show(25)\n",
    "hypo1_df2.orderBy('ddos_date').show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop the SparkContext\n",
    "note: don't run this block unless you actually want to stop your context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-04-23 11:41:07] Stopping Spark context.\n"
     ]
    }
   ],
   "source": [
    "print(\"[{}] Stopping Spark context.\".format(datetime.now().replace(microsecond=0)))\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
