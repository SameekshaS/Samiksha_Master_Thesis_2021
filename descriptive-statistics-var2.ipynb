{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAIR USE POLICY\n",
    "**Please do not leave your Jupyter lab server idle for extended periods of time.** The Jupyter process, active Python kernels, and especially running Spark contexts, claim a minimum amount of cluster resources. These add up and will get starve resources of others eventually. Leaving your environment idle for a few hours (e.g., over lunch) is fine. But letting it idle overnight or for multiple days in which you are not actively using the cluster is not. You can kill the server from your SSH session, by pressing ctrl+c repeatedly, or by selecting *File->Shutdown* from the menu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta, date\n",
    "import pprint\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F \n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Find Spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from datetime import datetime, timedelta, date\n",
    "import datetime as dt\n",
    "from datetime import timedelta as td\n",
    "import subprocess\n",
    "import re\n",
    "import pyarrow as pa\n",
    "import copy\n",
    "from pyspark.sql.types import DateType\n",
    "from string import digits\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import *  \n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F, Window\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import mean as _mean\n",
    "from pyspark.sql.window import Window as W\n",
    "\n",
    "\n",
    "from functools import reduce\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Spark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkConf\n",
    "APP_NAME = \"apwg-median-app\"\n",
    "\n",
    "spark_conf = pyspark.SparkConf().setAppName(APP_NAME).setMaster(\"yarn\").set(\n",
    "    \"spark.submit.deployMode\", \"client\"\n",
    ").set(\"spark.sql.parquet.binaryAsString\", \"true\"\n",
    ").set(\"spark.dynamicAllocation.maxExecutors\", \"16\"\n",
    ").set(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.0.0\"\n",
    ").set(\"spark.sql.debug.maxToStringFields\", \"1000\"\n",
    ").set(\"spark.executor.memory\", \"7G\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start SparkContext\n",
    "1. This may take a minute to complete\n",
    "2. You should not (and cannot) start two Spark contexts. If you accidentally run this cell twice or get stuck somehow, restart your Python kernel from the menu above.\n",
    "3. Please **stop your Spark context** when idling for extended periods of time (see code at bottom of notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-05-05 11:21:56] Starting Spark context.\n"
     ]
    }
   ],
   "source": [
    "print(\"[{}] Starting Spark context.\".format(datetime.now().replace(microsecond=0)))\n",
    "\n",
    "# SparkContext\n",
    "sc = pyspark.SparkContext(conf=spark_conf)\n",
    "\n",
    "# SQLContext\n",
    "\n",
    "sqlc = pyspark.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "APWG_CLEAN_DATA_CONVERTED_BASE = \"PATH TO DATA-RECORDS\"\n",
    "\n",
    "INTEREST_DATE_START = datetime(2017, 12, 18)\n",
    "INTEREST_DATE_END   = datetime(2019, 8, 16)\n",
    "\n",
    "\n",
    "# Read JSON files into Spark DF\n",
    "clean_mails_df = sqlc.read.json(APWG_CLEAN_DATA_CONVERTED_BASE, multiLine=True).withColumn(\n",
    "    \"parsed_date\", F.from_unixtime(F.col(\"date_received\")).cast(\"date\")\n",
    ").filter(\n",
    "    # Filter date range of interest\n",
    "    (F.col(\"parsed_date\") >= INTEREST_DATE_START.date().isoformat()) &\n",
    "    (F.col(\"parsed_date\") <= INTEREST_DATE_END.date().isoformat())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = clean_mails_df\n",
    "original_df = original_df.filter(original_df.language == \"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1_df = original_df   \n",
    "h1_df = h1_df.select('parsed_date', 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol='body_words', outputCol='words_clean')\n",
    "h2_df = remover.transform(original_df)\n",
    "\n",
    "h2_df = h2_df.drop(\"body_words\")\n",
    "\n",
    "word_list=['unsubscribed', 'hack', 'takedown', 'password', 'transparent',\\\n",
    "           'attempt', 'redirect', 'impersonate', 'network', 'obsolete', 'illegal', 'damage', 'edit',\\\n",
    "           'unauthenticated', 'initial', 'survey', 'collect', 'victim', 'detect', 'recharge', 'test',\\\n",
    "           'attachment', 'claim', 'profitable', 'virus', 'fraudulent', 'revalidation', 'link', 'description']\n",
    "\n",
    "#array_intersect function requires two arrays as arguments, create array from the list of given values:\n",
    "list_col = F.array(*[F.lit(cl) for cl in word_list])\n",
    "h2_df = h2_df.filter(F.size(F.array_intersect(F.col(\"words_clean\"), list_col)) > 0)\n",
    "h2_df = h2_df.select('parsed_date', 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3_df = original_df.filter(original_df.email_has_attachments == \"1\")\n",
    "h3_df = h3_df.select('parsed_date', 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# announcement date of ddos attack\n",
    "\n",
    "ddos_list = ['2018-01-17', '2018-01-27', '2018-02-08', '2018-03-01', '2018-03-06', '2018-05-14', '2018-05-24', '2018-07-30',\\\n",
    "             '2019-01-16','2019-01-31', '2019-02-23', '2019-03-22', '2019-04-16', '2019-06-12', '2019-08-08']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#partitionBy is used to shuffle data before applying the functions\n",
    "def hypo_1(df, day):\n",
    "    \n",
    "    \n",
    "    h1_df1 = (df.filter(f\"parsed_date < '{day}' and parsed_date > '{day}' - interval 20 days\")\n",
    "             .withColumn('rn', F.dense_rank().over(Window.orderBy(F.desc('parsed_date'))))\n",
    "             .filter('rn <= 7')\n",
    "             .drop('rn')\n",
    "             .withColumn('count_before', F.count('id').over(Window.partitionBy('parsed_date')))\n",
    "             .orderBy('parsed_date').withColumn('ddos_date', F.lit(day))\n",
    "          )\n",
    "\n",
    "    h1_df2 = (df.filter(f\"parsed_date < '{day}' + interval 20 days and parsed_date > '{day}'\")\n",
    "             .withColumn('rn', F.dense_rank().over(Window.orderBy('parsed_date')))\n",
    "             .filter('rn <= 7')\n",
    "             .drop('rn')\n",
    "             .withColumn('count_after', F.count('id').over(Window.partitionBy('parsed_date')))\n",
    "             .orderBy('parsed_date').withColumn('ddos_date', F.lit(day))\n",
    "          )\n",
    "    \n",
    "    return [h1_df1, h1_df2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypo_2(df, day):\n",
    "    \"\"\"\n",
    "    Example usage: df_list = hypo_2(df, '2017-12-18', 15)\n",
    "    Returns a list of 2 dataframes.\n",
    "    \"\"\"\n",
    "    h2_df1 = (df.filter(f\"parsed_date < '{day}' and parsed_date > '{day}' - interval 20 days\")\n",
    "             .withColumn('rn', F.dense_rank().over(Window.orderBy(F.desc('parsed_date'))))\n",
    "             .filter('rn <= 15')\n",
    "             .drop('rn')\n",
    "             .withColumn('count_before', F.count('id').over(Window.partitionBy('parsed_date')))\n",
    "             .orderBy('parsed_date').withColumn('ddos_date', F.lit(day))\n",
    "          )\n",
    "    \n",
    "    h2_df2 = (df.filter(f\"parsed_date < '{day}' + interval 20 days and parsed_date > '{day}'\")\n",
    "             .withColumn('rn', F.dense_rank().over(Window.orderBy('parsed_date')))\n",
    "             .filter('rn <= 15')\n",
    "             .drop('rn')\n",
    "             .withColumn('count_after', F.count('id').over(Window.partitionBy('parsed_date')))\n",
    "             .orderBy('parsed_date').withColumn('ddos_date', F.lit(day))\n",
    "          )\n",
    "    \n",
    "    return [h2_df1, h2_df2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypo_3(df, day):\n",
    "    \"\"\"\n",
    "    Example usage: df_list = hypo_3(df, '2017-12-18', 15)\n",
    "    Returns a list of 2 dataframes.\n",
    "    \"\"\"\n",
    "    h3_df1 = (df.filter(f\"parsed_date < '{day}' and parsed_date > '{day}' - interval 20 days\")\n",
    "             .withColumn('rn', F.dense_rank().over(Window.orderBy(F.desc('parsed_date'))))\n",
    "             .filter('rn <= 15')\n",
    "             .drop('rn')\n",
    "             .withColumn('count_before', F.count('id').over(Window.partitionBy('parsed_date')))\n",
    "             .orderBy('parsed_date').withColumn('ddos_date', F.lit(day))\n",
    "          )\n",
    "    \n",
    "    \n",
    "    h3_df2 = (df.filter(f\"parsed_date < '{day}' + interval 20 days and parsed_date > '{day}'\")\n",
    "             .withColumn('rn', F.dense_rank().over(Window.orderBy('parsed_date')))\n",
    "             .filter('rn <= 15')\n",
    "             .drop('rn')\n",
    "             .withColumn('count_after', F.count('id').over(Window.partitionBy('parsed_date')))\n",
    "             .orderBy('parsed_date').withColumn('ddos_date', F.lit(day))\n",
    "          )\n",
    "    \n",
    "    return [h3_df1, h3_df2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_h1_before_list = []\n",
    "statistics_h1_after_list = []\n",
    "\n",
    "for d in ddos_list:\n",
    "    h1_df1, h1_df2 = hypo_1(h1_df, d)\n",
    "    \n",
    "    df1 = h1_df1.dropDuplicates(['parsed_date', 'count_before'])\n",
    "    df2 = h1_df2.dropDuplicates(['parsed_date', 'count_after'])\n",
    "    \n",
    "    statistics_h1_before = df1.withColumn('median_h1_before', F.expr(\"percentile_approx(count_before, 0.5, 10) over ()\")).withColumn('ddos_date', F.lit(d))\n",
    "    statistics_h1_after = df2.withColumn('median_h1_after', F.expr(\"percentile_approx(count_after, 0.5, 10) over ()\")).withColumn('ddos_date', F.lit(d))\n",
    "    \n",
    "    \n",
    "    mean_before =  statistics_h1_before.groupBy().avg(\"count_before\").take(1)[0][0]\n",
    "    statistics_h1_before = statistics_h1_before.withColumn(\"mean_h1_before\", lit(mean_before))\n",
    "    \n",
    "    mean_after =  statistics_h1_after.groupBy().avg(\"count_after\").take(1)[0][0]\n",
    "    statistics_h1_after = statistics_h1_after.withColumn(\"mean_h1_after\", lit(mean_after))\n",
    "    \n",
    "    \n",
    "    statistics_h1_before = statistics_h1_before.withColumn(\"std_h1_before\", F.round(F.stddev(\"count_before\").over(Window.partitionBy('ddos_date')), 3))\n",
    "    statistics_h1_after = statistics_h1_after.withColumn(\"std_h1_after\", F.round(F.stddev(\"count_after\").over(Window.partitionBy('ddos_date')), 3))\n",
    "    \n",
    "    statistics_h1_before_list.append(statistics_h1_before)\n",
    "    statistics_h1_after_list.append(statistics_h1_after)\n",
    "    \n",
    "statistics_h1_before = reduce(DataFrame.unionAll, statistics_h1_before_list)\n",
    "statistics_h1_after = reduce(DataFrame.unionAll, statistics_h1_after_list)\n",
    "\n",
    "statistics_h1_before = statistics_h1_before.drop('parsed_date','id','count_before').dropDuplicates(['ddos_date', 'median_h1_before'])\n",
    "statistics_h1_after = statistics_h1_after.drop('parsed_date','id','count_after').dropDuplicates(['ddos_date', 'median_h1_after'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_h2_before_list = []\n",
    "statistics_h2_after_list = []\n",
    "\n",
    "for d in ddos_list:\n",
    "    h2_df1, h2_df2 = hypo_2(h2_df, d)\n",
    "    \n",
    "    df1 = h2_df1.dropDuplicates(['parsed_date', 'count_before'])\n",
    "    df2 = h2_df2.dropDuplicates(['parsed_date', 'count_after'])\n",
    "    \n",
    "    statistics_h2_before = df1.withColumn('median_h2_before', F.expr(\"percentile_approx(count_before, 0.5, 10) over ()\")).withColumn('ddos_date', F.lit(d))\n",
    "    statistics_h2_after = df2.withColumn('median_h2_after', F.expr(\"percentile_approx(count_after, 0.5, 10) over ()\")).withColumn('ddos_date', F.lit(d))\n",
    "    \n",
    "    \n",
    "    mean_before =  statistics_h2_before.groupBy().avg(\"count_before\").take(1)[0][0]\n",
    "    statistics_h2_before = statistics_h2_before.withColumn(\"mean_h2_before\", lit(mean_before))\n",
    "    \n",
    "    mean_after =  statistics_h2_after.groupBy().avg(\"count_after\").take(1)[0][0]\n",
    "    statistics_h2_after = statistics_h2_after.withColumn(\"mean_h2_after\", lit(mean_after))\n",
    "    \n",
    "    \n",
    "    statistics_h2_before = statistics_h2_before.withColumn(\"std_h2_before\", F.round(F.stddev(\"count_before\").over(Window.partitionBy('ddos_date')), 3))\n",
    "    statistics_h2_after = statistics_h2_after.withColumn(\"std_h2_after\", F.round(F.stddev(\"count_after\").over(Window.partitionBy('ddos_date')), 3))\n",
    "    \n",
    "    statistics_h2_before_list.append(statistics_h2_before)\n",
    "    statistics_h2_after_list.append(statistics_h2_after)\n",
    "    \n",
    "statistics_h2_before = reduce(DataFrame.unionAll, statistics_h2_before_list)\n",
    "statistics_h2_after = reduce(DataFrame.unionAll, statistics_h2_after_list)\n",
    "\n",
    "statistics_h2_before = statistics_h2_before.drop('parsed_date','id','count_before').dropDuplicates(['ddos_date', 'median_h2_before'])\n",
    "statistics_h2_after = statistics_h2_after.drop('parsed_date','id','count_after').dropDuplicates(['ddos_date', 'median_h2_after'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_h3_before_list = []\n",
    "statistics_h3_after_list = []\n",
    "\n",
    "for d in ddos_list:\n",
    "    h3_df1, h3_df2 = hypo_3(h3_df, d)\n",
    "    \n",
    "    df1 = h3_df1.dropDuplicates(['parsed_date', 'count_before'])\n",
    "    df2 = h3_df2.dropDuplicates(['parsed_date', 'count_after'])\n",
    "    \n",
    "    statistics_h3_before = df1.withColumn('median_h3_before', F.expr(\"percentile_approx(count_before, 0.5, 10) over ()\")).withColumn('ddos_date', F.lit(d))\n",
    "    statistics_h3_after = df2.withColumn('median_h3_after', F.expr(\"percentile_approx(count_after, 0.5, 10) over ()\")).withColumn('ddos_date', F.lit(d))\n",
    "    \n",
    "    \n",
    "    mean_before =  statistics_h3_before.groupBy().avg(\"count_before\").take(1)[0][0]\n",
    "    statistics_h3_before = statistics_h3_before.withColumn(\"mean_h3_before\", lit(mean_before))\n",
    "    \n",
    "    mean_after =  statistics_h3_after.groupBy().avg(\"count_after\").take(1)[0][0]\n",
    "    statistics_h3_after = statistics_h3_after.withColumn(\"mean_h3_after\", lit(mean_after))\n",
    "    \n",
    "    \n",
    "    statistics_h3_before = statistics_h3_before.withColumn(\"std_h3_before\", F.round(F.stddev(\"count_before\").over(Window.partitionBy('ddos_date')), 3))\n",
    "    statistics_h3_after = statistics_h3_after.withColumn(\"std_h3_after\", F.round(F.stddev(\"count_after\").over(Window.partitionBy('ddos_date')), 3))\n",
    "    \n",
    "    statistics_h3_before_list.append(statistics_h3_before)\n",
    "    statistics_h3_after_list.append(statistics_h3_after)\n",
    "    \n",
    "statistics_h3_before = reduce(DataFrame.unionAll, statistics_h3_before_list)\n",
    "statistics_h3_after = reduce(DataFrame.unionAll, statistics_h3_after_list)\n",
    "\n",
    "statistics_h3_before = statistics_h3_before.drop('parsed_date','id','count_before').dropDuplicates(['ddos_date', 'median_h3_before'])\n",
    "statistics_h3_after = statistics_h3_after.drop('parsed_date','id','count_after').dropDuplicates(['ddos_date', 'median_h3_after'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ddos_date: string, median_h3_after: bigint, mean_h3_after: double, std_h3_after: double]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics_h1_before.persist()\n",
    "statistics_h1_after.persist()\n",
    "statistics_h2_before.persist()\n",
    "statistics_h2_after.persist()\n",
    "statistics_h3_before.persist()\n",
    "statistics_h3_after.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+------------------+-------------+\n",
      "| ddos_date|median_h1_before|    mean_h1_before|std_h1_before|\n",
      "+----------+----------------+------------------+-------------+\n",
      "|2018-01-17|            1147|1119.7142857142858|       174.14|\n",
      "|2018-01-27|            1106|1074.4285714285713|      173.223|\n",
      "|2018-02-08|            1081| 1041.142857142857|      150.503|\n",
      "|2018-03-01|            1150| 1117.142857142857|       114.08|\n",
      "|2018-03-06|            1195|1135.4285714285713|      227.619|\n",
      "|2018-05-14|            1088|            1017.0|      209.015|\n",
      "|2018-05-24|            1246|1231.7142857142858|      179.391|\n",
      "|2018-07-30|            1352|1301.4285714285713|      306.922|\n",
      "|2019-01-16|              48|             649.0|      849.942|\n",
      "|2019-02-23|              70|             839.5|     1088.237|\n",
      "|2019-03-22|            1666|1577.6666666666667|     1430.547|\n",
      "|2019-04-16|            1673|1309.2857142857142|      779.417|\n",
      "|2019-08-08|             136|             782.0|      984.159|\n",
      "+----------+----------------+------------------+-------------+\n",
      "\n",
      "+----------+---------------+------------------+------------+\n",
      "| ddos_date|median_h1_after|     mean_h1_after|std_h1_after|\n",
      "+----------+---------------+------------------+------------+\n",
      "|2018-01-17|           1045| 1070.857142857143|     176.459|\n",
      "|2018-01-27|           1102|1106.5714285714287|     206.132|\n",
      "|2018-02-08|           1091| 1065.857142857143|     176.559|\n",
      "|2018-03-01|           1225|1154.5714285714287|     236.718|\n",
      "|2018-03-06|            101| 419.7142857142857|     567.176|\n",
      "|2018-05-14|           1213|1204.2857142857142|     161.439|\n",
      "|2018-05-24|           1266| 1227.857142857143|      184.11|\n",
      "|2018-07-30|           1210|1191.5714285714287|     193.404|\n",
      "|2019-01-16|             54|             922.5|    1228.244|\n",
      "|2019-01-31|             70|             881.0|     948.637|\n",
      "|2019-02-23|            105|            813.75|     856.242|\n",
      "|2019-03-22|           1728| 2330.714285714286|    1730.791|\n",
      "|2019-04-16|            231|           1003.75|     954.149|\n",
      "|2019-06-12|            192|             893.0|     991.364|\n",
      "|2019-08-08|              1|             340.0|     709.378|\n",
      "+----------+---------------+------------------+------------+\n",
      "\n",
      "+----------+----------------+------------------+-------------+\n",
      "| ddos_date|median_h2_before|    mean_h2_before|std_h2_before|\n",
      "+----------+----------------+------------------+-------------+\n",
      "|2018-01-17|             384|             403.0|       76.723|\n",
      "|2018-01-27|             422|             415.2|       67.039|\n",
      "|2018-02-08|             437|             434.0|       67.734|\n",
      "|2018-03-01|             413|407.93333333333334|       57.019|\n",
      "|2018-03-06|             428| 406.1333333333333|       71.093|\n",
      "|2018-05-14|             352| 366.1333333333333|      105.031|\n",
      "|2018-05-24|             426|             438.6|      119.334|\n",
      "|2018-07-30|             464| 490.6666666666667|      134.029|\n",
      "|2019-01-16|              24|             243.5|       310.42|\n",
      "|2019-02-23|              34|             314.5|      396.687|\n",
      "|2019-03-22|             673| 641.3333333333334|      575.154|\n",
      "|2019-04-16|             508|400.38461538461536|      295.335|\n",
      "|2019-08-08|             902|368.55555555555554|      346.052|\n",
      "+----------+----------------+------------------+-------------+\n",
      "\n",
      "+----------+---------------+------------------+------------+\n",
      "| ddos_date|median_h2_after|     mean_h2_after|std_h2_after|\n",
      "+----------+---------------+------------------+------------+\n",
      "|2018-01-17|            422|423.46666666666664|      61.607|\n",
      "|2018-01-27|            425| 422.3333333333333|      71.121|\n",
      "|2018-02-08|            425|             421.8|      65.529|\n",
      "|2018-03-01|             47|             222.8|     214.773|\n",
      "|2018-03-06|             34| 99.66666666666667|     169.168|\n",
      "|2018-05-14|            474|             492.2|      94.931|\n",
      "|2018-05-24|            465|488.26666666666665|      95.269|\n",
      "|2018-07-30|            462|470.26666666666665|      94.537|\n",
      "|2019-01-16|             17|             400.5|     542.351|\n",
      "|2019-01-31|             34|             357.5|      391.11|\n",
      "|2019-02-23|             51|             337.0|     350.067|\n",
      "|2019-03-22|            529| 734.6923076923077|     846.261|\n",
      "|2019-04-16|             74|            403.25|      399.21|\n",
      "|2019-06-12|             58|             341.5|      400.93|\n",
      "|2019-08-08|            116| 274.6666666666667|     378.801|\n",
      "+----------+---------------+------------------+------------+\n",
      "\n",
      "+----------+----------------+------------------+-------------+\n",
      "| ddos_date|median_h3_before|    mean_h3_before|std_h3_before|\n",
      "+----------+----------------+------------------+-------------+\n",
      "|2018-01-17|             132|136.66666666666666|       42.074|\n",
      "|2018-01-27|             115|140.73333333333332|       49.976|\n",
      "|2018-02-08|             135|159.53333333333333|       60.245|\n",
      "|2018-03-01|             120|             123.8|        38.38|\n",
      "|2018-03-06|             120|143.93333333333334|       72.805|\n",
      "|2018-05-14|              90|122.26666666666667|        62.63|\n",
      "|2018-05-24|             117|125.93333333333334|       63.002|\n",
      "|2018-07-30|             150|164.66666666666666|       68.201|\n",
      "|2019-01-16|               4|              59.5|       78.489|\n",
      "|2019-02-23|              13|             136.5|      174.655|\n",
      "|2019-03-22|             187|132.33333333333334|      105.249|\n",
      "|2019-04-16|             149|134.46153846153845|       96.629|\n",
      "|2019-08-08|              54|            110.25|       93.773|\n",
      "+----------+----------------+------------------+-------------+\n",
      "\n",
      "+----------+---------------+------------------+------------+\n",
      "| ddos_date|median_h3_after|     mean_h3_after|std_h3_after|\n",
      "+----------+---------------+------------------+------------+\n",
      "|2018-01-17|            135|151.26666666666668|      49.439|\n",
      "|2018-01-27|            140|157.86666666666667|      59.851|\n",
      "|2018-02-08|            124|             137.0|        37.2|\n",
      "|2018-03-01|             19| 90.93333333333334|     107.099|\n",
      "|2018-03-06|             12|35.666666666666664|      63.768|\n",
      "|2018-05-14|            117|149.86666666666667|     129.908|\n",
      "|2018-05-24|            133|157.13333333333333|     121.695|\n",
      "|2018-07-30|            123|131.46666666666667|      46.852|\n",
      "|2019-01-16|              3|             142.5|     197.283|\n",
      "|2019-01-31|             13|             139.5|     152.163|\n",
      "|2019-02-23|             11|              99.0|     104.256|\n",
      "|2019-03-22|            116|149.76923076923077|     103.191|\n",
      "|2019-04-16|             20|             130.5|     131.946|\n",
      "|2019-06-12|             17|             131.5|     161.927|\n",
      "|2019-08-08|             23|              59.0|      50.912|\n",
      "+----------+---------------+------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "statistics_h1_before.orderBy('ddos_date').show(25)\n",
    "statistics_h1_after.orderBy('ddos_date').show(25)\n",
    "statistics_h2_before.orderBy('ddos_date').show(25)\n",
    "statistics_h2_after.orderBy('ddos_date').show(25)\n",
    "statistics_h3_before.orderBy('ddos_date').show(25)\n",
    "statistics_h3_after.orderBy('ddos_date').show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop the SparkContext\n",
    "note: don't run this block unless you actually want to stop your context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-05-05 18:21:59] Stopping Spark context.\n"
     ]
    }
   ],
   "source": [
    "print(\"[{}] Stopping Spark context.\".format(datetime.now().replace(microsecond=0)))\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
