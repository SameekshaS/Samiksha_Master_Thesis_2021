{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAIR USE POLICY\n",
    "**Please do not leave your Jupyter lab server idle for extended periods of time.** The Jupyter process, active Python kernels, and especially running Spark contexts, claim a minimum amount of cluster resources. These add up and will get starve resources of others eventually. Leaving your environment idle for a few hours (e.g., over lunch) is fine. But letting it idle overnight or for multiple days in which you are not actively using the cluster is not. You can kill the server from your SSH session, by pressing ctrl+c repeatedly, or by selecting *File->Shutdown* from the menu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta, date\n",
    "import pprint\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F \n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Find Spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from datetime import datetime, timedelta, date\n",
    "import datetime as dt\n",
    "from datetime import timedelta as td\n",
    "import subprocess\n",
    "import re\n",
    "import pyarrow as pa\n",
    "import copy\n",
    "from pyspark.sql.types import DateType\n",
    "from string import digits\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import *  \n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F, Window\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import mean as _mean\n",
    "from pyspark.sql.window import Window as W\n",
    "import sys\n",
    "\n",
    "\n",
    "from functools import reduce\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "\n",
    "\n",
    "import json\n",
    "#json.loads('{\"\":\"\\\\ud800\"}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Spark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkConf\n",
    "APP_NAME = \"apwg-median-app\"\n",
    "\n",
    "spark_conf = pyspark.SparkConf().setAppName(APP_NAME).setMaster(\"yarn\").set(\n",
    "    \"spark.submit.deployMode\", \"client\"\n",
    ").set(\"spark.sql.parquet.binaryAsString\", \"true\"\n",
    ").set(\"spark.dynamicAllocation.maxExecutors\", \"16\"\n",
    ").set(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.0.0\"\n",
    ").set(\"spark.sql.debug.maxToStringFields\", \"1000\"\n",
    ").set(\"spark.executor.memory\", \"7G\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start SparkContext\n",
    "1. This may take a minute to complete\n",
    "2. You should not (and cannot) start two Spark contexts. If you accidentally run this cell twice or get stuck somehow, restart your Python kernel from the menu above.\n",
    "3. Please **stop your Spark context** when idling for extended periods of time (see code at bottom of notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-05-25 15:31:32] Starting Spark context.\n"
     ]
    }
   ],
   "source": [
    "print(\"[{}] Starting Spark context.\".format(datetime.now().replace(microsecond=0)))\n",
    "\n",
    "# SparkContext\n",
    "sc = pyspark.SparkContext(conf=spark_conf)\n",
    "\n",
    "# SQLContext\n",
    "\n",
    "sqlc = pyspark.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "APWG_CLEAN_DATA_CONVERTED_BASE = \"PATH TO DATA-RECORDS\"\n",
    "\n",
    "INTEREST_DATE_START = datetime(2017, 12, 18)\n",
    "INTEREST_DATE_END   = datetime(2019, 8, 16)\n",
    "\n",
    "\n",
    "# Read JSON files into Spark DF\n",
    "clean_mails_df = sqlc.read.json(APWG_CLEAN_DATA_CONVERTED_BASE, multiLine=True).withColumn(\n",
    "    \"parsed_date\", F.from_unixtime(F.col(\"date_received\")).cast(\"date\")\n",
    ").filter(\n",
    "    # Filter date range of interest\n",
    "    (F.col(\"parsed_date\") >= INTEREST_DATE_START.date().isoformat()) &\n",
    "    (F.col(\"parsed_date\") <= INTEREST_DATE_END.date().isoformat())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = clean_mails_df\n",
    "original_df = original_df.filter(original_df.language == \"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1_df = original_df   \n",
    "h1_df = h1_df.select('parsed_date', 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol='body_words', outputCol='words_clean')\n",
    "h2_df = remover.transform(original_df)\n",
    "\n",
    "h2_df = h2_df.drop(\"body_words\")\n",
    "\n",
    "word_list=['unsubscribed', 'hack', 'takedown', 'password', 'transparent',\\\n",
    "           'attempt', 'redirect', 'impersonate', 'network', 'obsolete', 'illegal', 'damage', 'edit',\\\n",
    "           'unauthenticated', 'initial', 'survey', 'collect', 'victim', 'detect', 'recharge', 'test',\\\n",
    "           'attachment', 'claim', 'profitable', 'virus', 'fraudulent', 'revalidation', 'link', 'description']\n",
    "\n",
    "#array_intersect function requires two arrays as arguments, create array from the list of given values:\n",
    "list_col = F.array(*[F.lit(cl) for cl in word_list])\n",
    "h2_df = h2_df.filter(F.size(F.array_intersect(F.col(\"words_clean\"), list_col)) > 0)\n",
    "h2_df = h2_df.select('parsed_date', 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3_df = original_df.filter(original_df.email_has_attachments == \"1\")\n",
    "h3_df = h3_df.select('parsed_date', 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# announcement date of ddos attack\n",
    "\n",
    "ddos_list = ['2018-01-17', '2018-01-27', '2018-02-08', '2018-03-01', '2018-03-06', '2018-05-14', '2018-05-24', '2018-07-30',\\\n",
    "             '2019-01-16','2019-01-31', '2019-02-23', '2019-03-22', '2019-04-16', '2019-06-12', '2019-08-08']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#partitionBy is used to shuffle data before applying the functions\n",
    "def hypo_1(df, day):\n",
    "    \n",
    "    \n",
    "    h1_df1 = (df.filter(f\"parsed_date < '{day}' and parsed_date > '{day}' - interval 10 days\")\n",
    "             .withColumn('rn', F.dense_rank().over(Window.orderBy(F.desc('parsed_date'))))\n",
    "             .filter('rn <= 3')\n",
    "             .drop('rn')\n",
    "             .withColumn('count_before', F.count('id').over(Window.partitionBy('parsed_date')))\n",
    "             .orderBy('parsed_date').withColumn('ddos_date', F.lit(day))\n",
    "          )\n",
    "    #h1_df1 = h1_df1.dropDuplicates(['parsed_date', 'count_before'])\n",
    "    h1_df2 = (df.filter(f\"parsed_date < '{day}' + interval 10 days and parsed_date > '{day}'\")\n",
    "             .withColumn('rn', F.dense_rank().over(Window.orderBy('parsed_date')))\n",
    "             .filter('rn <= 3')\n",
    "             .drop('rn')\n",
    "             .withColumn('count_after', F.count('id').over(Window.partitionBy('parsed_date')))\n",
    "             .orderBy('parsed_date').withColumn('ddos_date', F.lit(day))\n",
    "          )\n",
    "    #h1_df2 = h1_df2.dropDuplicates(['parsed_date', 'count_after'])\n",
    "    \n",
    "    return [h1_df1, h1_df2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypo_2(df, day):\n",
    "    \"\"\"\n",
    "    Example usage: df_list = hypo_2(df, '2017-12-18', 15)\n",
    "    Returns a list of 2 dataframes.\n",
    "    \"\"\"\n",
    "    h2_df1 = (df.filter(f\"parsed_date < '{day}' and parsed_date > '{day}' - interval 10 days\")\n",
    "             .withColumn('rn', F.dense_rank().over(Window.orderBy(F.desc('parsed_date'))))\n",
    "             .filter('rn <= 7')\n",
    "             .drop('rn')\n",
    "             .withColumn('count_before', F.count('id').over(Window.partitionBy('parsed_date')))\n",
    "             .orderBy('parsed_date').withColumn('ddos_date', F.lit(day))\n",
    "          )\n",
    "    #h2_df1 = h2_df1.dropDuplicates(['parsed_date', 'count_before'])\n",
    "    \n",
    "    h2_df2 = (df.filter(f\"parsed_date < '{day}' + interval 10 days and parsed_date > '{day}'\")\n",
    "             .withColumn('rn', F.dense_rank().over(Window.orderBy('parsed_date')))\n",
    "             .filter('rn <= 7')\n",
    "             .drop('rn')\n",
    "             .withColumn('count_after', F.count('id').over(Window.partitionBy('parsed_date')))\n",
    "             .orderBy('parsed_date').withColumn('ddos_date', F.lit(day))\n",
    "          )\n",
    "    #h2_df2 = h2_df2.dropDuplicates(['parsed_date', 'count_after'])\n",
    "    return [h2_df1, h2_df2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypo_3(df, day):\n",
    "    \"\"\"\n",
    "    Example usage: df_list = hypo_3(df, '2017-12-18', 15)\n",
    "    Returns a list of 2 dataframes.\n",
    "    \"\"\"\n",
    "    h3_df1 = (df.filter(f\"parsed_date < '{day}' and parsed_date > '{day}' - interval 10 days\")\n",
    "             .withColumn('rn', F.dense_rank().over(Window.orderBy(F.desc('parsed_date'))))\n",
    "             .filter('rn <= 7')\n",
    "             .drop('rn')\n",
    "             .withColumn('count_before', F.count('id').over(Window.partitionBy('parsed_date')))\n",
    "             .orderBy('parsed_date').withColumn('ddos_date', F.lit(day))\n",
    "          )\n",
    "    #h3_df1 = h3_df1.dropDuplicates(['parsed_date', 'count_before'])\n",
    "    \n",
    "    h3_df2 = (df.filter(f\"parsed_date < '{day}' + interval 10 days and parsed_date > '{day}'\")\n",
    "             .withColumn('rn', F.dense_rank().over(Window.orderBy('parsed_date')))\n",
    "             .filter('rn <= 7')\n",
    "             .drop('rn')\n",
    "             .withColumn('count_after', F.count('id').over(Window.partitionBy('parsed_date')))\n",
    "             .orderBy('parsed_date').withColumn('ddos_date', F.lit(day))\n",
    "          )\n",
    "    #h3_df2 = h3_df2.dropDuplicates(['parsed_date', 'count_after'])\n",
    "    \n",
    "    \n",
    "    return [h3_df1, h3_df2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_h1_before_list = []\n",
    "statistics_h1_after_list = []\n",
    "\n",
    "for d in ddos_list:\n",
    "    h1_df1, h1_df2 = hypo_1(h1_df, d)\n",
    "    \n",
    "    df1 = h1_df1.dropDuplicates(['parsed_date', 'count_before'])\n",
    "    df2 = h1_df2.dropDuplicates(['parsed_date', 'count_after'])\n",
    "    \n",
    "    statistics_h1_before = df1.withColumn('median_h1_before', F.expr(\"percentile_approx(count_before, 0.5, 10) over ()\")).withColumn('ddos_date', F.lit(d))\n",
    "    statistics_h1_after = df2.withColumn('median_h1_after', F.expr(\"percentile_approx(count_after, 0.5, 10) over ()\")).withColumn('ddos_date', F.lit(d))\n",
    "    \n",
    "    \n",
    "    mean_before =  statistics_h1_before.groupBy().avg(\"count_before\").take(1)[0][0]\n",
    "    statistics_h1_before = statistics_h1_before.withColumn(\"mean_h1_before\", lit(mean_before))\n",
    "    \n",
    "    mean_after =  statistics_h1_after.groupBy().avg(\"count_after\").take(1)[0][0]\n",
    "    statistics_h1_after = statistics_h1_after.withColumn(\"mean_h1_after\", lit(mean_after))\n",
    "    \n",
    "    \n",
    "    statistics_h1_before = statistics_h1_before.withColumn(\"std_h1_before\", F.round(F.stddev(\"count_before\").over(Window.partitionBy('ddos_date')), 3))\n",
    "    statistics_h1_after = statistics_h1_after.withColumn(\"std_h1_after\", F.round(F.stddev(\"count_after\").over(Window.partitionBy('ddos_date')), 3))\n",
    "    \n",
    "    statistics_h1_before_list.append(statistics_h1_before)\n",
    "    statistics_h1_after_list.append(statistics_h1_after)\n",
    "    \n",
    "statistics_h1_before = reduce(DataFrame.unionAll, statistics_h1_before_list)\n",
    "statistics_h1_after = reduce(DataFrame.unionAll, statistics_h1_after_list)\n",
    "\n",
    "statistics_h1_before = statistics_h1_before.drop('parsed_date','id','count_before').dropDuplicates(['ddos_date', 'median_h1_before'])\n",
    "statistics_h1_after = statistics_h1_after.drop('parsed_date','id','count_after').dropDuplicates(['ddos_date', 'median_h1_after'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_h2_before_list = []\n",
    "statistics_h2_after_list = []\n",
    "\n",
    "for d in ddos_list:\n",
    "    h2_df1, h2_df2 = hypo_2(h2_df, d)\n",
    "    \n",
    "    df1 = h2_df1.dropDuplicates(['parsed_date', 'count_before'])\n",
    "    df2 = h2_df2.dropDuplicates(['parsed_date', 'count_after'])\n",
    "    \n",
    "    statistics_h2_before = df1.withColumn('median_h2_before', F.expr(\"percentile_approx(count_before, 0.5, 10) over ()\")).withColumn('ddos_date', F.lit(d))\n",
    "    statistics_h2_after = df2.withColumn('median_h2_after', F.expr(\"percentile_approx(count_after, 0.5, 10) over ()\")).withColumn('ddos_date', F.lit(d))\n",
    "    \n",
    "    \n",
    "    mean_before =  statistics_h2_before.groupBy().avg(\"count_before\").take(1)[0][0]\n",
    "    statistics_h2_before = statistics_h2_before.withColumn(\"mean_h2_before\", lit(mean_before))\n",
    "    \n",
    "    mean_after =  statistics_h2_after.groupBy().avg(\"count_after\").take(1)[0][0]\n",
    "    statistics_h2_after = statistics_h2_after.withColumn(\"mean_h2_after\", lit(mean_after))\n",
    "    \n",
    "    \n",
    "    statistics_h2_before = statistics_h2_before.withColumn(\"std_h2_before\", F.round(F.stddev(\"count_before\").over(Window.partitionBy('ddos_date')), 3))\n",
    "    statistics_h2_after = statistics_h2_after.withColumn(\"std_h2_after\", F.round(F.stddev(\"count_after\").over(Window.partitionBy('ddos_date')), 3))\n",
    "    \n",
    "    statistics_h2_before_list.append(statistics_h2_before)\n",
    "    statistics_h2_after_list.append(statistics_h2_after)\n",
    "    \n",
    "statistics_h2_before = reduce(DataFrame.unionAll, statistics_h2_before_list)\n",
    "statistics_h2_after = reduce(DataFrame.unionAll, statistics_h2_after_list)\n",
    "\n",
    "statistics_h2_before = statistics_h2_before.drop('parsed_date','id','count_before').dropDuplicates(['ddos_date', 'median_h2_before'])\n",
    "statistics_h2_after = statistics_h2_after.drop('parsed_date','id','count_after').dropDuplicates(['ddos_date', 'median_h2_after'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_h3_before_list = []\n",
    "statistics_h3_after_list = []\n",
    "\n",
    "for d in ddos_list:\n",
    "    h3_df1, h3_df2 = hypo_3(h3_df, d)\n",
    "    \n",
    "    df1 = h3_df1.dropDuplicates(['parsed_date', 'count_before'])\n",
    "    df2 = h3_df2.dropDuplicates(['parsed_date', 'count_after'])\n",
    "    \n",
    "    statistics_h3_before = df1.withColumn('median_h3_before', F.expr(\"percentile_approx(count_before, 0.5, 10) over ()\")).withColumn('ddos_date', F.lit(d))\n",
    "    statistics_h3_after = df2.withColumn('median_h3_after', F.expr(\"percentile_approx(count_after, 0.5, 10) over ()\")).withColumn('ddos_date', F.lit(d))\n",
    "    \n",
    "    \n",
    "    mean_before =  statistics_h3_before.groupBy().avg(\"count_before\").take(1)[0][0]\n",
    "    statistics_h3_before = statistics_h3_before.withColumn(\"mean_h3_before\", lit(mean_before))\n",
    "    \n",
    "    mean_after =  statistics_h3_after.groupBy().avg(\"count_after\").take(1)[0][0]\n",
    "    statistics_h3_after = statistics_h3_after.withColumn(\"mean_h3_after\", lit(mean_after))\n",
    "    \n",
    "    \n",
    "    statistics_h3_before = statistics_h3_before.withColumn(\"std_h3_before\", F.round(F.stddev(\"count_before\").over(Window.partitionBy('ddos_date')), 3))\n",
    "    statistics_h3_after = statistics_h3_after.withColumn(\"std_h3_after\", F.round(F.stddev(\"count_after\").over(Window.partitionBy('ddos_date')), 3))\n",
    "    \n",
    "    statistics_h3_before_list.append(statistics_h3_before)\n",
    "    statistics_h3_after_list.append(statistics_h3_after)\n",
    "    \n",
    "statistics_h3_before = reduce(DataFrame.unionAll, statistics_h3_before_list)\n",
    "statistics_h3_after = reduce(DataFrame.unionAll, statistics_h3_after_list)\n",
    "\n",
    "statistics_h3_before = statistics_h3_before.drop('parsed_date','id','count_before').dropDuplicates(['ddos_date', 'median_h3_before'])\n",
    "statistics_h3_after = statistics_h3_after.drop('parsed_date','id','count_after').dropDuplicates(['ddos_date', 'median_h3_after'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+------------------+-------------+\n",
      "| ddos_date|median_h1_before|    mean_h1_before|std_h1_before|\n",
      "+----------+----------------+------------------+-------------+\n",
      "|2018-01-17|            1063|            1028.0|      219.602|\n",
      "|2018-01-27|            1126|1177.6666666666667|      107.277|\n",
      "|2018-02-08|            1140|1142.3333333333333|       86.524|\n",
      "|2018-03-01|            1187|            1181.0|       28.478|\n",
      "|2018-03-06|             938| 972.3333333333334|      250.273|\n",
      "|2018-05-14|             776| 858.3333333333334|      201.535|\n",
      "|2018-05-24|            1372|1391.3333333333333|       34.356|\n",
      "|2018-07-30|             994|1020.6666666666666|      225.187|\n",
      "|2019-02-23|              70|             839.5|     1088.237|\n",
      "|2019-03-22|            2962|            2962.0|          NaN|\n",
      "|2019-04-16|            1525|1156.6666666666667|       891.52|\n",
      "|2019-08-08|            2047|            1439.0|     1129.269|\n",
      "+----------+----------------+------------------+-------------+\n",
      "\n",
      "+----------+---------------+------------------+------------+\n",
      "| ddos_date|median_h1_after|     mean_h1_after|std_h1_after|\n",
      "+----------+---------------+------------------+------------+\n",
      "|2018-01-17|           1045|            1084.0|      74.586|\n",
      "|2018-01-27|           1271|            1135.0|     291.824|\n",
      "|2018-02-08|            914| 952.6666666666666|     130.374|\n",
      "|2018-03-01|            938|1040.6666666666667|     362.086|\n",
      "|2018-03-06|           1196| 858.6666666666666|     677.249|\n",
      "|2018-05-14|           1213|1268.6666666666667|     111.464|\n",
      "|2018-05-24|           1218|1160.6666666666667|     226.509|\n",
      "|2018-07-30|           1332|            1297.0|      78.581|\n",
      "|2019-01-31|             54|             922.5|    1228.244|\n",
      "|2019-02-23|           1435|            1050.0|     874.552|\n",
      "|2019-03-22|           1607|2654.6666666666665|    1948.616|\n",
      "|2019-04-16|            130|             935.0|    1138.442|\n",
      "|2019-08-08|              1| 88.66666666666667|     151.843|\n",
      "+----------+---------------+------------------+------------+\n",
      "\n",
      "+----------+----------------+------------------+-------------+\n",
      "| ddos_date|median_h2_before|    mean_h2_before|std_h2_before|\n",
      "+----------+----------------+------------------+-------------+\n",
      "|2018-01-17|             429|423.42857142857144|       88.096|\n",
      "|2018-01-27|             456|426.85714285714283|       66.699|\n",
      "|2018-02-08|             472|430.42857142857144|       77.691|\n",
      "|2018-03-01|             402|394.57142857142856|       67.594|\n",
      "|2018-03-06|             440|406.57142857142856|       77.985|\n",
      "|2018-05-14|             401|393.85714285714283|      130.383|\n",
      "|2018-05-24|             474|483.85714285714283|      108.447|\n",
      "|2018-07-30|             554|             506.0|      123.103|\n",
      "|2019-02-23|              34|             314.5|      396.687|\n",
      "|2019-03-22|            1200|            1200.0|          NaN|\n",
      "|2019-04-16|             529|             462.0|      245.649|\n",
      "|2019-08-08|             340|            525.75|      397.164|\n",
      "+----------+----------------+------------------+-------------+\n",
      "\n",
      "+----------+---------------+------------------+------------+\n",
      "| ddos_date|median_h2_after|     mean_h2_after|std_h2_after|\n",
      "+----------+---------------+------------------+------------+\n",
      "|2018-01-17|            422|413.85714285714283|      65.892|\n",
      "|2018-01-27|            437| 428.7142857142857|      77.228|\n",
      "|2018-02-08|            425|424.14285714285717|      79.876|\n",
      "|2018-03-01|            466|434.42857142857144|      97.877|\n",
      "|2018-03-06|             47|177.28571428571428|     231.463|\n",
      "|2018-05-14|            482| 465.7142857142857|      67.426|\n",
      "|2018-05-24|            465|506.85714285714283|     112.815|\n",
      "|2018-07-30|            512|468.14285714285717|      97.244|\n",
      "|2019-01-31|             17|             400.5|     542.351|\n",
      "|2019-02-23|            605| 432.3333333333333|     359.568|\n",
      "|2019-03-22|            568|1067.5714285714287|    1035.462|\n",
      "|2019-04-16|             50|             358.5|     436.285|\n",
      "|2019-08-08|            116| 274.6666666666667|     378.801|\n",
      "+----------+---------------+------------------+------------+\n",
      "\n",
      "+----------+----------------+------------------+-------------+\n",
      "| ddos_date|median_h3_before|    mean_h3_before|std_h3_before|\n",
      "+----------+----------------+------------------+-------------+\n",
      "|2018-01-17|             132|141.71428571428572|       52.217|\n",
      "|2018-01-27|             135|146.85714285714286|       49.764|\n",
      "|2018-02-08|             140|152.28571428571428|       66.875|\n",
      "|2018-03-01|             120|119.85714285714286|       43.129|\n",
      "|2018-03-06|             127|             164.0|       96.338|\n",
      "|2018-05-14|             129|126.71428571428571|       71.721|\n",
      "|2018-05-24|             121|145.14285714285714|       83.149|\n",
      "|2018-07-30|             159|166.85714285714286|       51.731|\n",
      "|2019-02-23|              13|             136.5|      174.655|\n",
      "|2019-03-22|             187|             187.0|          NaN|\n",
      "|2019-04-16|             204|             162.6|       89.456|\n",
      "|2019-08-08|              54|             117.5|      105.529|\n",
      "+----------+----------------+------------------+-------------+\n",
      "\n",
      "+----------+---------------+------------------+------------+\n",
      "| ddos_date|median_h3_after|     mean_h3_after|std_h3_after|\n",
      "+----------+---------------+------------------+------------+\n",
      "|2018-01-17|            142|147.28571428571428|      50.046|\n",
      "|2018-01-27|            154|169.42857142857142|      64.187|\n",
      "|2018-02-08|            165|148.14285714285714|      36.325|\n",
      "|2018-03-01|            165|181.28571428571428|      94.136|\n",
      "|2018-03-06|             19| 64.28571428571429|      87.658|\n",
      "|2018-05-14|            121|122.71428571428571|      38.239|\n",
      "|2018-05-24|            134|172.57142857142858|     179.688|\n",
      "|2018-07-30|            143|132.42857142857142|      41.713|\n",
      "|2019-01-31|              3|             142.5|     197.283|\n",
      "|2019-02-23|            179|128.33333333333334|     105.553|\n",
      "|2019-03-22|            189|             179.0|     107.236|\n",
      "|2019-04-16|             13|             134.0|      171.12|\n",
      "|2019-08-08|             23|              59.0|      50.912|\n",
      "+----------+---------------+------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "statistics_h1_before.orderBy('ddos_date').show(25)\n",
    "statistics_h1_after.orderBy('ddos_date').show(25)\n",
    "statistics_h2_before.orderBy('ddos_date').show(25)\n",
    "statistics_h2_after.orderBy('ddos_date').show(25)\n",
    "statistics_h3_before.orderBy('ddos_date').show(25)\n",
    "statistics_h3_after.orderBy('ddos_date').show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop the SparkContext\n",
    "note: don't run this block unless you actually want to stop your context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-05-05 21:09:14] Stopping Spark context.\n"
     ]
    }
   ],
   "source": [
    "print(\"[{}] Stopping Spark context.\".format(datetime.now().replace(microsecond=0)))\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
